{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the dataset\n",
    "dataset = pd.read_csv(\"/content/50combined.csv\")  # Update with your dataset path\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Define the columns you want to impute\n",
    "columns_to_impute = ['Sample_50', 'Sample_45', 'Sample_46', 'Sample_47', 'Sample_48', 'Sample_49']  # Add more columns as needed\n",
    "\n",
    "# Instantiate the KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Group rows by label and fill NaN values within each group for each specified column\n",
    "for column in columns_to_impute:\n",
    "    grouped = dataset.groupby('Label')[column]\n",
    "    for label, group in grouped:\n",
    "        # Create a mask to identify NaN values for this group\n",
    "        nan_mask = group.isnull()\n",
    "        if nan_mask.any():\n",
    "            # Exclude the 'Label' column and impute NaN values using KNNImputer\n",
    "            features = group.drop(columns=['Label']).values.reshape(-1, 1)\n",
    "            features_imputed = imputer.fit_transform(features)\n",
    "\n",
    "            # Update the DataFrame with imputed values using the mask\n",
    "            dataset.loc[group.index, column] = features_imputed.flatten()\n",
    "\n",
    "# Extract features (all columns except the last one)\n",
    "X = dataset.iloc[:, 1:].values\n",
    "\n",
    "# Extract labels (last column)\n",
    "y = dataset.iloc[:, 0].values\n",
    "\n",
    "# Encode labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Calculate mean and standard deviation for each feature\n",
    "standard_scaler = StandardScaler()\n",
    "X_normalized = standard_scaler.fit_transform(X)\n",
    "\n",
    "# Scale the features using MinMaxScaler\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_scaled = min_max_scaler.fit_transform(X_normalized)\n",
    "\n",
    "# Convert data to PyTorch tensors with float32 data type and move to GPU\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32).to(device)\n",
    "y_tensor = torch.tensor(y, dtype=torch.int64).to(device)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=0)\n",
    "\n",
    "# Define DataLoader for batching the data\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# Define a CNN-LSTM model (combine convolutional and LSTM layers)\n",
    "class CNNLSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_channels, hidden_size, num_layers, num_classes, kernel_size=3, dropout=0.2):\n",
    "        super(CNNLSTMClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_channels, out_channels=16, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(2)\n",
    "        self.lstm = nn.LSTM(16, hidden_size, num_layers)\n",
    "        self.fc1 = nn.Linear(hidden_size, 64)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "        self.relu = nn.SELU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1).repeat(1, 50, 1)  # Add a channel dimension\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.dropout(x[:, -1, :])\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define an RNN model\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.2):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.relu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1).repeat(1, 50, 1)\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        out = self.fc(self.relu(out))\n",
    "        return out\n",
    "\n",
    "# Instantiate both models\n",
    "input_channels = X_train.shape[1]\n",
    "hidden_size_cnn_lstm = 128\n",
    "hidden_size_rnn = 128\n",
    "num_layers = 3\n",
    "num_classes = len(label_encoder.classes_)\n",
    "cnn_lstm_model = CNNLSTMClassifier(input_channels, hidden_size_cnn_lstm, num_layers, num_classes).to(device)\n",
    "rnn_model = RNNClassifier(input_channels, hidden_size_rnn, num_layers, num_classes).to(device)\n",
    "\n",
    "# Define L2 regularization parameter\n",
    "weight_decay = 7e-5\n",
    "\n",
    "# Define loss functions and optimizers for both models with L2 regularization\n",
    "criterion_cnn_lstm = nn.CrossEntropyLoss(ignore_index=num_classes)\n",
    "optimizer_cnn_lstm = optim.RAdam(cnn_lstm_model.parameters(), lr=0.006, weight_decay=weight_decay)\n",
    "criterion_rnn = nn.CrossEntropyLoss()\n",
    "optimizer_rnn = optim.RAdam(rnn_model.parameters(), lr=0.001, weight_decay=weight_decay)\n",
    "\n",
    "# Train both models\n",
    "num_epochs = 125\n",
    "best_loss_cnn_lstm = float('inf')\n",
    "best_loss_rnn = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    cnn_lstm_model.train()\n",
    "    rnn_model.train()\n",
    "    running_loss_cnn_lstm = 0.0\n",
    "    running_loss_rnn = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Train CNN-LSTM model\n",
    "        optimizer_cnn_lstm.zero_grad()\n",
    "        outputs_cnn_lstm = cnn_lstm_model(inputs)\n",
    "        loss_cnn_lstm = criterion_cnn_lstm(outputs_cnn_lstm, labels)\n",
    "        loss_cnn_lstm.backward()\n",
    "        optimizer_cnn_lstm.step()\n",
    "        running_loss_cnn_lstm += loss_cnn_lstm.item() * inputs.size(0)\n",
    "\n",
    "        # Train RNN model\n",
    "        optimizer_rnn.zero_grad()\n",
    "        outputs_rnn = rnn_model(inputs)\n",
    "        loss_rnn = criterion_rnn(outputs_rnn, labels)\n",
    "        loss_rnn.backward()\n",
    "        optimizer_rnn.step()\n",
    "        running_loss_rnn += loss_rnn.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss_cnn_lstm = running_loss_cnn_lstm / len(train_loader.dataset)\n",
    "    epoch_loss_rnn = running_loss_rnn / len(train_loader.dataset)\n",
    "\n",
    "    # Print progress\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], CNN-LSTM Loss: {epoch_loss_cnn_lstm:.4f}, RNN Loss: {epoch_loss_rnn:.4f}\")\n",
    "\n",
    "# Evaluate both models on the test set\n",
    "cnn_lstm_model.eval()\n",
    "rnn_model.eval()\n",
    "combined_probs = []\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Get probabilities from CNN-LSTM model\n",
    "        outputs_cnn_lstm = cnn_lstm_model(inputs)\n",
    "        probs_cnn_lstm = nn.functional.softmax(outputs_cnn_lstm, dim=1)\n",
    "\n",
    "        # Get probabilities from RNN model\n",
    "        outputs_rnn = rnn_model(inputs)\n",
    "        probs_rnn = nn.functional.softmax(outputs_rnn, dim=1)\n",
    "\n",
    "        # Combine probabilities from both models\n",
    "        combined_probs_batch = (probs_cnn_lstm + probs_rnn) / 2\n",
    "        combined_probs.append(combined_probs_batch.cpu().numpy())\n",
    "\n",
    "        # Get predicted labels from combined probabilities\n",
    "        _, predicted = torch.max(combined_probs_batch, 1)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Ensemble Test Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# Flatten and concatenate probabilities from all batches\n",
    "combined_probs = np.concatenate(combined_probs, axis=0)\n",
    "y_pred_combined = np.argmax(combined_probs, axis=1)\n",
    "y_true_combined = y_test.cpu().numpy()\n",
    "\n",
    "# Print classification report for ensemble model\n",
    "print(\"Classification Report for Ensemble Model:\")\n",
    "print(classification_report(y_true_combined, y_pred_combined))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save pkl files of models, scalers, and encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from google.colab import files\n",
    "\n",
    "# Save CNN-LSTM model\n",
    "torch.save(cnn_lstm_model, 'cnn_lstm_model_14.pkl')\n",
    "files.download('cnn_lstm_model_14.pkl')\n",
    "\n",
    "# Save RNN model\n",
    "torch.save(rnn_model, 'rnn_model_14.pkl')\n",
    "files.download('rnn_model_14.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from google.colab import files\n",
    "\n",
    "joblib.dump(standard_scaler, 'standard_scaler.pkl')\n",
    "files.download('standard_scaler.pkl')\n",
    "\n",
    "joblib.dump(min_max_scaler, 'min_max_scaler.pkl')\n",
    "files.download('min_max_scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Assuming label_encoder is your LabelEncoder object\n",
    "joblib.dump(label_encoder, 'label_encoder.pkl')\n",
    "files.download('label_encoder.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "rnn_model = torch.load('/content/rnn_model_14.pkl', map_location = torch.device('cpu'))\n",
    "cnn_lstm_model = torch.load('/content/cnn_lstm_model_14.pkl', map_location = torch.device('cpu'))\n",
    "\n",
    "# Load the label encoder\n",
    "label_encoder = joblib.load('/content/label_encoder.pkl')\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Load the CSV file\n",
    "data = pd.read_csv(\"/content/combined.csv\")\n",
    "\n",
    "# Separate features and labels\n",
    "X = data.drop(columns=['Label'])  # Assuming 'label' is the column name for the target variable\n",
    "y = data['Label']\n",
    "\n",
    "# Calculate mean and standard deviation for each feature\n",
    "means = X.mean(axis=0)\n",
    "stds = X.std(axis=0)\n",
    "\n",
    "# Mean normalization\n",
    "X_normalized = (X - means) / stds\n",
    "\n",
    "# # Scale the features using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Convert the preprocessed data to PyTorch tensor\n",
    "input_tensor = torch.tensor(X_scaled, dtype=torch.float32)\n",
    "\n",
    "# Create a DataLoader for batch processing\n",
    "batch_size = 1  # Since we're processing one sample at a time\n",
    "data_loader = DataLoader(TensorDataset(input_tensor), batch_size=batch_size)\n",
    "\n",
    "# Set models to evaluation mode\n",
    "rnn_model.eval()\n",
    "cnn_lstm_model.eval()\n",
    "\n",
    "# Create an empty list to store predictions\n",
    "predictions = []\n",
    "\n",
    "# Iterate over the data loader\n",
    "for batch in data_loader:\n",
    "    input_data = batch[0]  # Extract input data from the batch\n",
    "\n",
    "    # Generate predictions for the current batch\n",
    "    with torch.no_grad():\n",
    "        rnn_output = rnn_model(input_data)\n",
    "        probs_rnn = nn.functional.softmax(rnn_output, dim=1)\n",
    "        cnn_output = cnn_lstm_model(input_data)\n",
    "        probs_cnn_lstm = nn.functional.softmax(cnn_output, dim=1)\n",
    "\n",
    "        # Combine probabilities from both models\n",
    "        combined_probs_batch = (probs_cnn_lstm + probs_rnn) / 2\n",
    "\n",
    "        # Get predicted labels from combined probabilities\n",
    "        _, predicted = torch.max(combined_probs_batch, 1)\n",
    "        predictions.extend(predicted.tolist())\n",
    "\n",
    "# Convert predicted labels to original class labels\n",
    "predicted_labels = label_encoder.inverse_transform(predictions)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Accuracy Score : \", accuracy_score(y, predicted_labels))\n",
    "print(classification_report(y, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Get the confusion matrix\n",
    "cf_matrix = confusion_matrix(y, predicted_labels)\n",
    "print(cf_matrix)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming cf_matrix is your confusion matrix\n",
    "plt.figure(figsize=(15, 15))  # Set the size of the figure\n",
    "\n",
    "# Plot the heatmap with real class labels\n",
    "sns.heatmap(cf_matrix, annot=True, cmap='Blues', xticklabels=y.unique(), yticklabels=y.unique())\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scaler pkl usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2808.3357036  1991.06076235 2004.43617054 2391.57298502 2615.0798059\n",
      "  2911.96386608 3193.9724723  3291.10043641 3289.97540208 3161.22147282\n",
      "  2946.46491897 2684.95693838 2330.94613483 2032.8120365  1809.93023469\n",
      "  1718.55244606 2003.81115146 1778.8042848  1720.30249947 1809.30521561\n",
      "  2021.56169317 2311.94555498 2736.5835139  3085.71916868 3283.60020753\n",
      "  3298.6006653  3104.96975616 2791.71019623 2424.57399213 2156.69081698\n",
      "  1851.68150884 1725.05264443 1811.18027284 2082.06353954 2440.44947661\n",
      "  2812.33582568 3132.47059542 3261.34952849 3289.97540208 3100.34461501\n",
      "  2753.4590289  2394.57307657 2037.43717765 2238.56831568 2512.9516892\n",
      "  2721.83306375 2677.33170568 2632.58033998 2650.33088168 2763.20932646]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:288: UserWarning: Trying to unpickle estimator StandardScaler from version 1.2.2 when using version 1.2.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.63627834, -0.9391976 , -0.91700509, -0.1514977 ,  0.28804801,\n",
       "         0.86950533,  1.43314642,  1.63216697,  1.63449589,  1.37603956,\n",
       "         0.94302144,  0.42177054, -0.26949151, -0.84285131, -1.27550831,\n",
       "        -1.44853995, -0.90187408, -1.33514749, -1.45810778, -1.28080957,\n",
       "        -0.86624281, -0.30280856,  0.50679354,  1.17742525,  1.5476338 ,\n",
       "         1.57831979,  1.21313501,  0.61186037, -0.09796614, -0.61802767,\n",
       "        -1.21124198, -1.46187604, -1.29752336, -0.78017043, -0.09002249,\n",
       "         0.63781174,  1.26054945,  1.51288611,  1.55859204,  1.18632027,\n",
       "         0.51037645, -0.19918615, -0.90954094, -0.51303657,  0.01808797,\n",
       "         0.42817971,  0.33993158,  0.24908887,  0.27535853,  0.49641438]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Load the scaler\n",
    "standard_scaler = joblib.load(r'C:\\Users\\Admin\\Downloads\\Anusha\\AIML\\Final Project\\real time pipeline\\models\\standard_scaler.pkl')\n",
    "\n",
    "np_array = np.array([[2808.335703604236,1991.0607623523667,2004.436170537431,2391.572985015412,2615.0798059022795,2911.9638660847804,3193.972472304453,3291.100436414685,3289.975402081362,3161.2214728232675,2946.4649189733577,2684.956938383129,2330.946134830775,2032.8120365001373,1809.930234687338,1718.5524460585343,2003.8111514633624,1778.8042847987304,1720.302499465926,1809.3052156132692,2021.5616931669056,2311.9455549790946,2736.583513901181,3085.7191686758024,3283.6002075258643,3298.600665303507,3104.9697561571093,2791.710196234016,2424.5739921262248,2156.6908169804988,1851.6815088351088,1725.0526444288462,1811.1802728354749,2082.063539536729,2440.4494766075622,2812.3358256782744,3132.47059541612,3261.3495284890287,3289.975402081362,3100.344615009003,2753.4590289010284,2394.57307657094,2037.4371776482435,2238.5683156834625,2512.9516891995,2721.8330637531662,2677.3317056794945,2632.580339976196,2650.330881679739,2763.209326456496]])\n",
    "print(np_array)\n",
    "\n",
    "X_new_scaled = standard_scaler.transform(np_array)\n",
    "\n",
    "X_new_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical Ensemble (SVC + KNN + RF + XGBOOST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Load the dataset\n",
    "dataset = pd.read_csv(\"/content/50combined.csv\")  # Update with your dataset path\n",
    "\n",
    "# Define the columns you want to impute\n",
    "columns_to_impute = ['Sample_50', 'Sample_45', 'Sample_46', 'Sample_47', 'Sample_48', 'Sample_49', 'Sample_44']  # Add more columns as needed\n",
    "\n",
    "# Instantiate the KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Group rows by label and fill NaN values within each group for each specified column\n",
    "for column in columns_to_impute:\n",
    "    grouped = dataset.groupby('Label')[column]\n",
    "    for label, group in grouped:\n",
    "        # Create a mask to identify NaN values for this group\n",
    "        nan_mask = group.isnull()\n",
    "        if nan_mask.any():\n",
    "            # Exclude the 'Label' column and impute NaN values using KNNImputer\n",
    "            features = group.drop(columns=['Label']).values.reshape(-1, 1)\n",
    "            features_imputed = imputer.fit_transform(features)\n",
    "\n",
    "            # Update the DataFrame with imputed values using the mask\n",
    "            dataset.loc[group.index, column] = features_imputed.flatten()\n",
    "\n",
    "# Extract features (all columns except the last one)\n",
    "X = dataset.iloc[:, 1:].values\n",
    "\n",
    "# Extract labels (last column)\n",
    "y = dataset.iloc[:, 0].values\n",
    "\n",
    "# Encode labels to integers\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X)\n",
    "\n",
    "# Scale the features using MinMaxScaler\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_scaled = min_max_scaler.fit_transform(X_standardized)\n",
    "\n",
    "base_classifiers = [\n",
    "    ('xgb', XGBClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        reg_lambda=0.5\n",
    "    )),\n",
    "    ('svm', SVC(\n",
    "        C=3.0,\n",
    "        kernel='rbf',\n",
    "        gamma='scale',\n",
    "        decision_function_shape = 'ovo'\n",
    "    )),\n",
    "    ('knn', KNeighborsClassifier(\n",
    "        n_neighbors=5,\n",
    "        weights='distance',\n",
    "        algorithm='kd_tree'\n",
    "    )),\n",
    "    ('rf', RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        random_state=42,\n",
    "        bootstrap = True\n",
    "    ))\n",
    "]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define meta-classifier (Softmax Regression)\n",
    "meta_classifier = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam', max_iter=500)\n",
    "\n",
    "# Create stacking classifier with cross-validation\n",
    "stacking_classifier = StackingClassifier(estimators=base_classifiers, final_estimator=meta_classifier, cv=5, verbose=2)\n",
    "\n",
    "# Train stacking classifier\n",
    "stacking_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = stacking_classifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Evaluate the predictions\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluate the predictions\n",
    "report = classification_report(y_test, predictions)\n",
    "print(\"Classification Report:\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from google.colab import files\n",
    "\n",
    "# Save the stacking classifier\n",
    "joblib.dump(stacking_classifier, 'stacking_classifier_22.pkl')\n",
    "files.download('stacking_classifier_22.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load the stacking classifier\n",
    "stacking_classifier = joblib.load('/content/stacking_classifier_22.pkl')\n",
    "\n",
    "# Load the test data\n",
    "test_data = pd.read_csv(\"/content/combined.csv\")  # Replace \"path_to_test_data.csv\" with the path to your test data file\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Define the columns you want to impute\n",
    "columns_to_impute = ['Sample_50']  # Add more columns as needed\n",
    "\n",
    "# Instantiate the KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# Group rows by label and fill NaN values within each group for each specified column\n",
    "for column in columns_to_impute:\n",
    "    grouped = test_data.groupby('Label')[column]\n",
    "    for label, group in grouped:\n",
    "        # Create a mask to identify NaN values for this group\n",
    "        nan_mask = group.isnull()\n",
    "        if nan_mask.any():\n",
    "            # Exclude the 'Label' column and impute NaN values using KNNImputer\n",
    "            features = group.drop(columns=['Label']).values.reshape(-1, 1)\n",
    "            features_imputed = imputer.fit_transform(features)\n",
    "\n",
    "            # Update the DataFrame with imputed values using the mask\n",
    "            test_data.loc[group.index, column] = features_imputed.flatten()\n",
    "\n",
    "# Separate features and labels\n",
    "X_test = test_data.drop(columns=['Label'])\n",
    "y_test = test_data['Label']\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_standardized = scaler.fit_transform(X_test)\n",
    "\n",
    "# Scale the features using MinMaxScaler\n",
    "min_max_scaler = MinMaxScaler()\n",
    "X_scaled = min_max_scaler.fit_transform(X_standardized)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = stacking_classifier.predict(X_scaled)\n",
    "predictions = label_encoder.inverse_transform(predictions)\n",
    "\n",
    "# Print classification report and accuracy score\n",
    "print(\"Accuracy Score : \", accuracy_score(y_test, predictions))\n",
    "print(classification_report(y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
